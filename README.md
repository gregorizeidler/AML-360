# ğŸ”¬ AML 360Âº â€” Advanced Anti-Money Laundering Detection System

<div align="center">

**ğŸ† Hybrid Architecture: Graph + Sequence + Bayesian + Statistical**

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![Framework](https://img.shields.io/badge/ML-Ensemble-green.svg)](src/models/)
[![Statistics](https://img.shields.io/badge/Statistics-Advanced-red.svg)](src/statistics/)
[![API](https://img.shields.io/badge/API-FastAPI-teal.svg)](src/api/)
[![Database](https://img.shields.io/badge/DB-BigQuery-orange.svg)](sql/)

*A scientifically rigorous, production-ready AML system combining cutting-edge machine learning, advanced statistics, and causal inference*

</div>

---

## ğŸ¯ **System Overview**

The AML 360Âº system represents the pinnacle of anti-money laundering technology, integrating **advanced statistical methods**, **modern machine learning**, and **rigorous scientific validation** to detect financial crimes with unprecedented accuracy and explainability.

### **ğŸ”¥ Key Differentiators**
- **ğŸ“Š Advanced Statistics**: EVT, GARCH, WOE/IV, Causal Inference
- **ğŸ§  Hybrid ML**: Graph Neural Networks + Sequential Models + Bayesian Fusion  
- **ğŸ”¬ Scientific Rigor**: Monte Carlo validation, statistical testing, uncertainty quantification
- **âš¡ Production Ready**: Real-time API, monitoring, explainability
- **ğŸ›ï¸ Regulatory Compliant**: Audit-ready documentation and interpretability

---

## ğŸ§® **Mathematical Foundation**

Our system is built on solid mathematical principles:

```mathematical
Cost Function: J = c_FN Ã— FN + c_FP Ã— FP + c_rev Ã— Review_Time

Bayesian Fusion: P(SAR|x,D) = âˆ‘ P(SAR|x,M_k,D) Â· P(M_k|D)
                                k=1

EVT Modeling: F_u(x) = 1 - (1 + Î¾(x/Ïƒ))^(-1/Î¾)

ATE Estimation: Ï„Ì‚_DR = 1/n âˆ‘[Î¼Ì‚â‚(Xáµ¢) - Î¼Ì‚â‚€(Xáµ¢) + Táµ¢(Yáµ¢-Î¼Ì‚â‚(Xáµ¢))/e(Xáµ¢) - (1-Táµ¢)(Yáµ¢-Î¼Ì‚â‚€(Xáµ¢))/(1-e(Xáµ¢))]
```

---

## ğŸ—ï¸ **System Architecture**

```mermaid
graph TB
    subgraph "Data Layer"
        A[Transactions<br/>PIX, Cards, TED] --> B[BigQuery<br/>Data Warehouse]
        C[External Data<br/>PEP, Sanctions] --> B
        D[Behavioral Data<br/>Device, IP, Geo] --> B
    end
    
    subgraph "Feature Engineering"
        B --> E[Statistical Features<br/>EVT, GARCH, WOE/IV]
        B --> F[Graph Features<br/>Centrality, Community]
        B --> G[Sequential Features<br/>Patterns, Burstiness]
        B --> H[Behavioral Features<br/>Velocity, Geo-entropy]
    end
    
    subgraph "ML Models"
        E --> I[Regulatory Scorecard<br/>Logistic + WOE]
        E --> J[Advanced Tabular<br/>LightGBM + Focal Loss]
        F --> K[Graph Neural Network<br/>GraphSAGE + GAT]
        G --> L[Sequential Model<br/>LSTM + Transformers]
        H --> M[Anomaly Detection<br/>Isolation Forest + LOF]
    end
    
    subgraph "Ensemble & Decision"
        I --> N[Bayesian Fusion<br/>MCMC Weighting]
        J --> N
        K --> N
        L --> N
        M --> N
        N --> O[Risk Score<br/>+ Explainability]
    end
    
    subgraph "Production"
        O --> P[FastAPI<br/>Real-time Scoring]
        P --> Q[Monitoring<br/>Drift Detection]
        P --> R[Case Management<br/>Analyst Dashboard]
    end
```

---

## ğŸ“Š **Feature Engineering Pipeline**

### **Statistical Methods Implemented**

| Method | Formula | Application |
|--------|---------|-------------|
| **Extreme Value Theory** | $F_u(x) = 1 - (1 + \xi \frac{x}{\sigma})^{-1/\xi}$ | Tail risk analysis |
| **GARCH Volatility** | $\sigma_t^2 = \omega + \alpha \epsilon_{t-1}^2 + \beta \sigma_{t-1}^2$ | Transaction volatility |
| **Weight of Evidence** | $WOE_i = \ln(\frac{G_i/G_T}{B_i/B_T})$ | Feature binning |
| **Fano Factor** | $F = \frac{\text{Var}(N)}{\text{E}[N]}$ | Burstiness detection |
| **PageRank** | $PR(v) = \frac{1-d}{N} + d \sum_{u \in M(v)} \frac{PR(u)}{L(u)}$ | Network importance |

### **Feature Categories**

```sql
-- Advanced Statistical Features (571 lines of SQL)
CREATE OR REPLACE TABLE `aml.ml_features` AS
SELECT 
    -- Behavioral (24h/7d/30d windows)
    tx_count_24h, velocity_ratio_24h_7d, unique_payees_24h,
    
    -- Statistical (EVT, GARCH, Burstiness)  
    evt_p95_threshold, amount_above_evt, fano_factor,
    
    -- Graph (Community, Centrality)
    out_degree, pagerank_proxy, community_bridge,
    
    -- Sequential (Patterns, Structuring)
    potential_smurfing, potential_layering, max_run_length,
    
    -- WOE Encoded  
    woe_amount, woe_volume, woe_diversity,
    
    -- Composite Risk
    composite_risk_score
FROM feature_engineering_pipeline;
```

---

## ğŸ“ **Project Structure**

```
AML 360Âº/
â”œâ”€â”€ ğŸ“š docs/                           # Scientific Documentation
â”‚   â”œâ”€â”€ mathematical-foundations.md    # Mathematical foundations
â”‚   â”œâ”€â”€ technical-architecture.md      # Detailed system design  
â”‚   â””â”€â”€ implementation-roadmap.md      # RACI matrix & timeline
â”‚
â”œâ”€â”€ ğŸ”¬ src/statistics/                 # Advanced Statistical Framework
â”‚   â”œâ”€â”€ statistical_tests.py          # Comprehensive statistical testing
â”‚   â”œâ”€â”€ causal_inference.py           # Propensity scores, ATE estimation
â”‚   â””â”€â”€ monte_carlo.py                 # Uncertainty quantification
â”‚
â”œâ”€â”€ ğŸ¤– src/models/                     # ML Model Implementations  
â”‚   â””â”€â”€ ensemble.py                   # Hybrid ensemble system
â”‚
â”œâ”€â”€ âš¡ src/api/                        # Production API
â”‚   â””â”€â”€ main.py                       # FastAPI with monitoring
â”‚
â”œâ”€â”€ ğŸ” src/features/                   # Feature Engineering
â”‚   â””â”€â”€ feature_extractor.py          # Statistical features
â”‚
â”œâ”€â”€ ğŸ’¾ sql/                           # BigQuery Pipeline
â”‚   â””â”€â”€ feature_engineering.sql       # Advanced features
â”‚
â”œâ”€â”€ ğŸ“Š notebooks/                     # Analysis Notebooks
â”‚   â”œâ”€â”€ 01_AML_EDA_and_Training.ipynb # Model training & evaluation
â”‚   â””â”€â”€ 02_Statistical_Validation.ipynb # Scientific validation
â”‚
â”œâ”€â”€ ğŸ³ docker/                        # Production Deployment
â”‚   â”œâ”€â”€ Dockerfile                    # Multi-stage optimized build
â”‚   â””â”€â”€ docker-compose.yml            # Full stack with monitoring
â”‚
â”œâ”€â”€ âš™ï¸ config/                        # System Configuration
â”‚   â””â”€â”€ config.yaml                   # Comprehensive settings
â”‚
â””â”€â”€ ğŸš€ setup.py                       # Automated setup & testing
```

---

## ğŸ§  **Machine Learning Models**

### **1. Regulatory Scorecard (Explainable)**
```python
class RegulatoryScorecardModel:
    """Logistic Regression with WOE encoding - fully explainable"""
    
    def calculate_woe_iv(self, feature, target):
        # WOE = ln(Distribution_Goods / Distribution_Bads)
        # IV = Î£(Dist_Goods - Dist_Bads) Ã— WOE
        return woe_dict, information_value
    
    def get_reason_codes(self, prediction):
        return ["High transaction velocity", "Unusual geographic pattern", ...]
```

### **2. Advanced Tabular Models**
```python
class AdvancedTabularModel:
    """LightGBM with Focal Loss for imbalanced data"""
    
    def focal_loss(self, y_pred, y_true, alpha=0.25, gamma=2.0):
        # FL(p_t) = -Î±_t(1-p_t)^Î³ log(p_t)
        p = 1 / (1 + np.exp(-y_pred))
        alpha_t = alpha * y_true + (1 - alpha) * (1 - y_true)  
        pt = p * y_true + (1 - p) * (1 - y_true)
        return alpha_t * (1 - pt) ** gamma * (-np.log(pt + 1e-8))
```

### **3. Bayesian Ensemble Fusion**
```python
class BayesianEnsemble:
    """Probabilistic model combination with uncertainty quantification"""
    
    def fit(self, model_predictions, targets):
        with pm.Model() as fusion_model:
            # Dirichlet prior for model weights
            weights = pm.Dirichlet('weights', a=np.ones(len(models)))
            
            # Weighted combination
            combined_logits = pm.math.dot(predictions, weights)
            
            # MCMC sampling for optimal weights
            trace = pm.sample(2000, tune=1000, chains=2)
```

---

## ğŸ”¬ **Scientific Validation Framework**

### **Advanced Statistical Testing**

| Test | Purpose | Implementation |
|------|---------|----------------|
| **Population Stability Index** | Model stability | `statistical_tests.py` |
| **Kolmogorov-Smirnov** | Distribution drift | Bootstrap CI + significance |
| **Jensen-Shannon Divergence** | Feature drift | Monte Carlo validation |
| **Extreme Value Theory** | Tail risk analysis | GPD fitting + goodness-of-fit |
| **GARCH Effects** | Volatility clustering | LM test + parameter estimation |
| **Causal Inference** | Treatment effects | Propensity scores + doubly robust |

### **Performance Targets**
```
ğŸ“ˆ MODEL PERFORMANCE TARGETS
=============================
PR-AUC:              Target: >0.75
ROC-AUC:             Target: >0.85 
Recall@20%:          Target: >80%
Precision:           Target: >70%
False Positive Rate: Target: <5%

âš¡ SYSTEM PERFORMANCE TARGETS
==============================
API Latency (p95):   Target: <200ms
Throughput:          Target: >5K TPS
Uptime:              Target: >99.9%
```

---

## ğŸ”§ **Quick Start**

### **1. Installation & Setup**
```bash
# Clone and setup
git clone <repository>
cd PROJETCT/
python setup.py  # Automated setup with dependency check
```

### **2. Model Training**
```python
# Start Jupyter environment
jupyter lab

# Open: notebooks/01_AML_EDA_and_Training.ipynb
# This trains the complete ensemble system with scientific validation
```

### **3. Production Deployment**
```bash
# Start complete stack with monitoring
docker-compose -f docker/docker-compose.yml up

# Access points:
# API: http://localhost:8000
# Docs: http://localhost:8000/docs  
# Grafana: http://localhost:3000
```

---

## âš¡ **Production API**

### **Real-time Scoring**
```bash
curl -X POST "http://localhost:8000/score" \
  -H "Content-Type: application/json" \
  -d '{
    "transaction_id": "tx_123456",
    "payer_id": "user_789", 
    "amount": 9950.00,
    "timestamp": "2024-01-15T14:30:00Z",
    "channel": "PIX"
  }'
```

**Response Schema:**
```json
{
  "risk_score": "float (0-1)",
  "risk_level": "string (LOW/MEDIUM/HIGH)", 
  "confidence": "float (0-1)",
  "reason_codes": [
    {
      "feature": "string",
      "contribution": "float",
      "description": "string"
    }
  ],
  "processing_time_ms": "integer"
}
```

---

<div align="center">

## ğŸš€ **Ready to Transform AML Operations?**

**Experience the power of scientifically-proven technology**

```bash
python setup.py && echo "ğŸ‰ Welcome to the future of AML detection!"
```



</div>
